version: "3.9"

services:

  fastapi:
    build:
      context: ./fastapi
      dockerfile: Dockerfile
    command: uvicorn main:app --host 0.0.0.0 --port 8000
    volumes:
      - ./uploaded_files:/app/uploaded_files
    environment:
      - LLM_1GPU_REPLICAS=${LLM_1GPU_REPLICAS}
      - LLM_2GPU_REPLICAS=${LLM_2GPU_REPLICAS}
      - LLM_4GPU_REPLICAS=${LLM_4GPU_REPLICAS}
      - LLM_8GPU_REPLICAS=${LLM_8GPU_REPLICAS}
      - ALM_REPLICAS=${ALM_REPLICAS}
      - ASR_REPLICAS=${ASR_REPLICAS}
      - CODE_LLM_REPLICAS=${CODE_LLM_REPLICAS}
      - EMB_REPLICAS=${EMB_REPLICAS}
      - VLM_REPLICAS=${VLM_REPLICAS}
      - QWQ_REPLICAS=${QWQ_REPLICAS}
      - DeepSeek_R1_REPLICAS=${DeepSeek_R1_REPLICAS}
      - REMOTE_EMB=${REMOTE_EMB}
      - REMOTE_EMB_BASE=${REMOTE_EMB_BASE}
      - REMOTE_LLM=${REMOTE_LLM}
      - REMOTE_LLM_BASE=${REMOTE_LLM_BASE}
    ports:
      - 8000:8000
    restart: always
    profiles: ["host"]


  nginx:
    build:
      context: ./nginx
      dockerfile: Dockerfile
    env_file:
      - ./env
    ports:
      - 80:80
    restart: always
    profiles: ["host"]


  vlm-qwen2-vl-7b:
    image: christtzm/nv_dev:cuda12.4-vllm-v2.1
    container_name: vlm-qwen2-vl-7b
    runtime: nvidia
    ipc: host
    volumes:
      - ${MODEL_PATH:-/home/tzm/tzm/models}:/models
      - ./deploy_script:/app
    ports:
      - "8022:8022"
    command: zsh -c "${VLM_COMMAND:-zsh app/start_qwen2_vl_7B.sh}"
    restart: always
    deploy:
      replicas: ${VLM_REPLICAS:-0}
    profiles: ["host"]


  llm-qwen2_5-7b:
    image: christtzm/nv_dev:cuda12.4-vllm-v1.0
    container_name: llm-qwen2_5-7b
    runtime: nvidia
    ipc: host
    volumes:
      - ${MODEL_PATH:-/home/tzm/tzm/models}:/models
      - ./deploy_script:/app
    ports:
      - "8012:8012"
    command: zsh -c "zsh app/start_qwen2_5_7B.sh"
    restart: always
    deploy:
      replicas: ${LLM_1GPU_REPLICAS:-0}
    profiles: ["host"]

  llm-qwen2_5-72b-int4-2gpu:
    image: christtzm/nv_dev:cuda12.4-vllm-v1.0
    container_name: llm-qwen2_5-72b-int4-2gpu
    runtime: nvidia
    ipc: host
    volumes:
      - ${MODEL_PATH:-/home/tzm/tzm/models}:/models
      - ./deploy_script:/app
    ports:
      - "8012:8012"
    command: zsh -c "zsh app/start_qwen2_5_72B_int4_2gpu.sh"
    restart: always
    deploy:
      replicas: ${LLM_2GPU_REPLICAS:-0}
    profiles: ["host"]

  llm-qwen2_5-72b-int4:
    image: christtzm/nv_dev:cuda12.4-vllm-v1.0
    container_name: llm-qwen2_5-72b-int4
    runtime: nvidia
    ipc: host
    volumes:
      - ${MODEL_PATH:-/home/tzm/tzm/models}:/models
      - ./deploy_script:/app
    ports:
      - "8012:8012"
    command: zsh -c "zsh app/start_qwen2_5_72B_int4.sh"
    restart: always
    deploy:
      replicas: ${LLM_4GPU_REPLICAS:-0}
    profiles: ["host"]


  llm-qwen2_5-72b:
    image: christtzm/nv_dev:cuda12.4-vllm-v1.0
    container_name: llm-qwen2_5-72b
    runtime: nvidia
    ipc: host
    volumes:
      - ${MODEL_PATH:-/home/tzm/tzm/models}:/models
      - ./deploy_script:/app
    ports:
      - "8012:8012"
    command: zsh -c "zsh app/start_qwen2_5_72B.sh"
    restart: always
    deploy:
      replicas: ${LLM_8GPU_REPLICAS:-0}
    profiles: ["host"]


  llm-qwen-QwQ-32B:
    image: christtzm/nv_dev:cuda12.4-vllm-v1.0
    container_name: llm-qwen-qwq-32b
    runtime: nvidia
    ipc: host
    volumes:
      - ${MODEL_PATH:-/home/tzm/tzm/models}:/models
      - ./deploy_script:/app
    ports:
      - "8012:8012"
    command: zsh -c "zsh app/start_qwen_QwQ_32B.sh"
    restart: always
    deploy:
      replicas: ${QWQ_REPLICAS:-0}
    profiles: ["host"]

  deepseek-r1:
    image: christtzm/nv_dev:cuda12.4-vllm-v3.0-r1
    container_name: deepseek-r1
    runtime: nvidia
    ipc: host
    volumes:
      - ${MODEL_PATH:-/home/tzm/tzm/models}:/models
      - ./deploy_script:/app
    ports:
      - "8072:8072"
    command: zsh -c "${DeepSeek_R1_COMMAND:-zsh app/start_deepseek_r1_qwen_32b.sh}"
    restart: always
    deploy:
      replicas: ${DeepSeek_R1_REPLICAS:-0}
    profiles: ["host"]

  llm-qwen2_5-code:
    image: christtzm/nv_dev:cuda12.4-vllm-v2.0
    container_name: llm-qwen2_5-code
    runtime: nvidia
    ipc: host
    volumes:
      - ${MODEL_PATH:-/home/tzm/tzm/models}:/models
      - ./deploy_script:/app
    ports:
      - "8012:8012"
    command: zsh -c "${CODE_LLM_COMMAND:-zsh app/start_qwen2_5_code_7B.sh}"
    restart: always
    deploy:
      replicas: ${CODE_LLM_REPLICAS:-0}
    profiles: ["host"]


  llm-qwen2-audio-7b:
    image: christtzm/nv_dev:cuda12.4-vllm-v1.0
    container_name: llm-qwen2-audio-7b
    runtime: nvidia
    ipc: host
    volumes:
      - ${MODEL_PATH:-/home/tzm/tzm/models}:/models
      - ./deploy_script:/app
    ports:
      - "8032:8032"
    command: zsh -c "zsh app/start_qwen2_audio_7B.sh"
    restart: always
    deploy:
      replicas: ${ALM_REPLICAS:-0}
    profiles: ["host"]


  whisper-large-v3:
    image: christtzm/nv_dev:cuda12.4-whisper-v1.0
    container_name: whisper-large-v3
    runtime: nvidia
    ipc: host
    volumes:
      - ${MODEL_PATH:-/home/tzm/tzm/models}:/models
      - ./deploy_script:/src
    ports:
      - "8132:8132"
    command: zsh -c "zsh src/start_whisper_large_v3.sh"
    restart: always
    deploy:
      replicas: ${ASR_REPLICAS:-0}
    profiles: ["host"]


  embed-gte-qwen2-7b:
    image: christtzm/nv_dev:cuda12.4-sglang-v1.0
    container_name: embed-gte-qwen2-7b
    runtime: nvidia
    ipc: host
    volumes:
      - ${MODEL_PATH:-/home/tzm/tzm/models}:/models
      - ./deploy_script:/app
    ports:
      - "8112:8112"
    command: zsh -c "zsh app/start_gte_qwen2_7B.sh"
    restart: always
    deploy:
      replicas: ${EMB_REPLICAS:-0}
    profiles: ["host"]


  ray-head:
    image: qwen3-deploy-with_parser:latest
    container_name: ray-head
    network_mode: host
    runtime: nvidia
    shm_size: 10.24g
    environment:
      - RAY_HEAD_IP=${RAY_HEAD_IP}
      - VLLM_HOST_IP=${RAY_HEAD_IP}
      - GLOO_SOCKET_IFNAME=${HEAD_GLOO_SOCKET_IFNAME}
      - TP_SOCKET_IFNAME=${HEAD_TP_SOCKET_IFNAME}
      - NCCL_SOCKET_IFNAME=${HEAD_NCCL_SOCKET_IFNAME}
    entrypoint:
      - zsh
      - -c
      - |
        source ~/.zshrc
        ray start --head --port=6379 --node-ip-address=$RAY_HEAD_IP --block &
        exec "$@"
    command:
      - zsh
      - -c
      - /app/start_qwen3_235B_FP8.sh
    volumes:
      - ${MODEL_PATH:-/data/tzm_data/models}:/root/.cache/huggingface
      - ./deploy_script:/app
    restart: always
    deploy:
      replicas: ${RAY_HEAD:-0}
    profiles: ["head"]


  ray-worker:
    image: qwen3-deploy-with_parser:latest
    container_name: ray-worker
    network_mode: host
    runtime: nvidia
    shm_size: 10.24g
    entrypoint: ["zsh", "-c", "source ~/.zshrc && ray start --block --address=${RAY_HEAD_IP}:6379"]
    volumes:
      - ${MODEL_PATH:-/data/tzm_data/models}:/root/.cache/huggingface
    environment:
      - VLLM_HOST_IP=${RAY_WORKER_IP}
      - GLOO_SOCKET_IFNAME=${WORKER_GLOO_SOCKET_IFNAME}
      - TP_SOCKET_IFNAME=${WORKER_TP_SOCKET_IFNAME}
      - NCCL_SOCKET_IFNAME=${WORKER_NCCL_SOCKET_IFNAME}
    restart: always
    deploy:
      replicas: ${RAY_WORKER:-0}
    profiles: ["worker"]

